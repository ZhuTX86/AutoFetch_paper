import os
import re
import sys
import time
from datetime import datetime
from serpapi import GoogleSearch
from googletrans import Translator

# ================= é…ç½®åŒºåŸŸ =================
SEARCH_QUERY = 'optimization (Muon OR Gluon OR Shampoo OR "linear minimization oracle" OR LMO)'
YEAR_LOW = datetime.now().year - 2
YEAR_HIGH = datetime.now().year
FILE_NAME = "papers.md"
# ===========================================

def clean_text(text):
    if not text: return "N/A"
    return text.replace("\n", " ").replace("|", "ï½œ").strip()

def translate_to_zh(text):
    """å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘ä¸ºä¸­æ–‡"""
    if not text or text == "N/A":
        return "æš‚æ— æ‘˜è¦"
    try:
        # å®ä¾‹åŒ–ç¿»è¯‘å™¨
        translator = Translator()
        # å°è¯•ç¿»è¯‘
        result = translator.translate(text, dest='zh-cn')
        return result.text
    except Exception as e:
        print(f"ç¿»è¯‘è®°å½•æ—¶å‡ºç°å°æ’æ›²: {e}")
        return "ï¼ˆç¿»è¯‘æš‚æ—¶ä¸å¯ç”¨ï¼‰"

def load_existing_links(file_path):
    """è¯»å–æ—§æ–‡ä»¶ï¼Œæå–å·²å­˜åœ¨çš„è®ºæ–‡é“¾æ¥é˜²æ­¢é‡å¤"""
    if not os.path.exists(file_path):
        return set()
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
        return set(re.findall(r'\[æŸ¥çœ‹è¯¦æƒ…\]\((https?://[^\s)]+)\)', content))

def fetch_scholar_data():
    api_key = os.getenv("SERPAPI_KEY")
    if not api_key:
        print("é”™è¯¯: ç¯å¢ƒå˜é‡ SERPAPI_KEY æœªè®¾ç½®")
        sys.exit(1)

    params = {
        "engine": "google_scholar",
        "q": SEARCH_QUERY,
        "as_ylo": YEAR_LOW,
        "as_yhi": YEAR_HIGH,
        "num": "5",  # æ¯æ¬¡æŠ“å–å‰20æ¡
        "hl": "zh-CN",
        "api_key": api_key
    }

    try:
        search = GoogleSearch(params)
        return search.get_dict().get("organic_results", [])
    except Exception as e:
        print(f"SerpApi è¯·æ±‚å¤±è´¥: {e}")
        return []

def main():
    print(f"ğŸš€ å¯åŠ¨è‡ªåŠ¨åŒ–è¿½è¸ªä»»åŠ¡: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    existing_links = load_existing_links(FILE_NAME)
    raw_papers = fetch_scholar_data()
    
    if not raw_papers:
        print("ğŸ’¡ æœªæ£€ç´¢åˆ°æ–°å†…å®¹ã€‚")
        return

    new_rows = []
    for item in raw_papers:
        link = item.get("link")
        title = item.get("title", "Untitled")
        
        # åŸºç¡€è¿‡æ»¤
        if not link or "[CITATION]" in title.upper() or "[B]" in title.upper():
            continue
        # å¢é‡å»é‡
        if link in existing_links:
            continue
            
        print(f"ğŸ“ æ­£åœ¨å¤„ç†æ–°æ–‡çŒ®: {title[:50]}...")
        
        # è·å–å¹¶ç¿»è¯‘æ‘˜è¦
        snippet_en = item.get("snippet", "")
        snippet_zh = translate_to_zh(snippet_en)
        
        # æ ¼å¼åŒ–æ•°æ®ï¼ˆæ”¯æŒ Markdown æ¢è¡Œæ’ç‰ˆï¼‰
        clean_title = clean_text(title)
        year_info = item.get("publication_info", {}).get("summary", "N/A")
        
        # æ’ç‰ˆä¼˜åŒ–ï¼šä¸­æ–‡åœ¨å‰ï¼Œè‹±æ–‡åœ¨åå¹¶ç¼©å°
        combined_snippet = f"{clean_text(snippet_zh)}<br><small>åŸæ–‡: {clean_text(snippet_en)}</small>"
        
        row = f"| {year_info} | **{clean_title}** | {combined_snippet} | [æŸ¥çœ‹è¯¦æƒ…]({link}) |"
        new_rows.append(row)
        
        # ç¨å¾®æš‚åœé˜²æ­¢ç¿»è¯‘æ¥å£è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)

    if not new_rows:
        print("ğŸ’¡ æ£€ç´¢åˆ°çš„æ–‡çŒ®åº“ä¸­å‡å·²å­˜åœ¨ã€‚")
        return

    # æ„å»ºæ–‡ä»¶å†…å®¹
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')
    table_header = "| å‘è¡¨æ—¶é—´/æ¥æº | è®ºæ–‡é¢˜ç›® | æ‘˜è¦ (ä¸­/è‹±) | é“¾æ¥ |\n| :--- | :--- | :--- | :--- |\n"
    title_section = f"# ğŸ“ è‡ªåŠ¨æ–‡çŒ®è¿½è¸ªæŠ¥å‘Š\n\n> æœ€åæ›´æ–°: `{timestamp}` | æœç´¢è¯: `{SEARCH_QUERY}`\n\n"

    if not os.path.exists(FILE_NAME):
        final_content = title_section + table_header + "\n".join(new_rows)
    else:
        with open(FILE_NAME, "r", encoding="utf-8") as f:
            old_lines = f.readlines()
        
        # å®šä½è¡¨æ ¼å†…å®¹å¼€å§‹çš„è¡Œ
        header_index = 0
        for i, line in enumerate(old_lines):
            if "| :--- |" in line:
                header_index = i + 1
                break
        
        old_rows_part = "".join(old_lines[header_index:]) if header_index > 0 else ""
        final_content = title_section + table_header + "\n".join(new_rows) + "\n" + old_rows_part

    with open(FILE_NAME, "w", encoding="utf-8") as f:
        f.write(final_content)
    
    print(f"âœ¨ ä»»åŠ¡å®Œæˆ! æœ¬æ¬¡æ–°å¢ {len(new_rows)} ç¯‡æ–‡çŒ®ã€‚")

if __name__ == "__main__":
    main()

    




